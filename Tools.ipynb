{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "CAS on Advanced Machine Learning <br>\n",
    "Data Science Lab, University of Bern, 2024<br>\n",
    "Prepared by Dr. Mykhailo Vladymyrov.\n",
    "\n",
    "</p>\n",
    "\n",
    "This work is licensed under a <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on colab:\n",
    "# !pip install mlflow\n",
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import einops as eo\n",
    "import pathlib as pl\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from time import time as timer\n",
    "#import umap\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Audio\n",
    "import IPython\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "\n",
    "import mlflow \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    root_path = '/content/drive/My Drive/Colab Notebooks/CAS_AML_M3'\n",
    "    root_path = pl.Path(root_path)\n",
    "    root_path.mkdir(exist_ok=True, parents=True)\n",
    "else:\n",
    "    root_path = pl.Path.cwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LAYERS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: 60000 total images and labels\n",
      "Test dataset shape: 10000 total images and labels\n"
     ]
    }
   ],
   "source": [
    "# create simple torch model for digit classsification\n",
    "\n",
    "m, s = 0.5, 0.5\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((m,), (s,)),\n",
    "                                transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST_data/', download=True, train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "# Download and load the test data\n",
    "validset = datasets.FashionMNIST('~/.pytorch/FMNIST_data/', download=True, train=False, transform=transform)\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print('Train dataset shape:', len(trainset), 'total images and labels')\n",
    "print('Test dataset shape:', len(validset), 'total images and labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hiddens, n_output):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.ls = []\n",
    "        n_prev = n_input\n",
    "        for i, n_out in enumerate(n_hiddens):\n",
    "          l = nn.Linear(n_prev, n_out)  # for hidden layer we create a linear projection form n_prev features to n_out features\n",
    "          n_prev = n_out\n",
    "          self.add_module(f'lin_{i}_{n_out}', l)\n",
    "          self.ls.append(l)\n",
    "\n",
    "        self.lout = nn.Linear(n_prev, n_output)  # also we need the output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for li in self.ls:  # for each layer we apply the linear projection and the activation fuinction (ReLU)\n",
    "          h = li(h)\n",
    "          h = torch.relu(h)\n",
    "\n",
    "        logits = self.lout(h)\n",
    "        # Apply softmax activation per row, to get the class pseudoprobabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Prediction: argmax for classification\n",
    "        pred = torch.argmax(probs, dim=1)  # find the element with highest value in each row\n",
    "\n",
    "        return logits, probs, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = []\n",
    "model = MyModel(n_input=784, n_hiddens=n_hidden, n_output=10)  # 784 input features for 28x28 images, 10 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npars(model):\n",
    "    \"\"\"\n",
    "    Returns the total number of parameters in the given PyTorch model.\n",
    "    For each parmaeter in the model, the function multiplies the elements of the shape\n",
    "    of the parameter tensor to get the total number of parameters.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to get the number of parameters for.\n",
    "    \n",
    "    Returns:\n",
    "        int: The total number of parameters in the model.\n",
    "    \"\"\"\n",
    "    n = np.sum([np.prod(list(p.shape)) for p in model.parameters()])\n",
    "    return int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7850"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_npars(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_correct = 0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        output, prob, pred = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()    # compute gradients\n",
    "        optimizer.step()   # update parameters with gradients\n",
    "        running_loss += loss.item()\n",
    "        correct = torch.sum(pred == labels)\n",
    "        n_correct += correct\n",
    "    return running_loss / len(trainloader), n_correct/len(trainloader)/trainloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            output, prob, pred = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "            correct = torch.sum(pred == labels)\n",
    "            n_correct += correct\n",
    "    return running_loss / len(testloader), n_correct/len(testloader)/testloader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    valid_loss, valid_acc = test(model, valid_loader, criterion)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Valid Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Acc: {valid_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/23 17:33:59 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///d:\\docs\\DSL\\Teaching\\CAS_AML\\CAS_AML_M3\\mlflow\n",
      "Epoch 1/5, Valid Loss: 0.5475, Valid Loss: 0.4713, Train Acc: 0.8086, Valid Acc: 0.8311\n",
      "Epoch 2/5, Valid Loss: 0.4242, Valid Loss: 0.4257, Train Acc: 0.8482, Valid Acc: 0.8454\n",
      "Epoch 3/5, Valid Loss: 0.3883, Valid Loss: 0.4156, Train Acc: 0.8603, Valid Acc: 0.8464\n",
      "Epoch 4/5, Valid Loss: 0.3683, Valid Loss: 0.4020, Train Acc: 0.8677, Valid Acc: 0.8547\n",
      "Epoch 5/5, Valid Loss: 0.3511, Valid Loss: 0.4004, Train Acc: 0.8722, Valid Acc: 0.8550\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 7\n",
    "\n",
    "mlflow_log_path = root_path / 'mlflow'\n",
    "mlflow_log_path.mkdir(exist_ok=True, parents=True)\n",
    "mlflow_log_path =  str(mlflow_log_path)  # path to mlflow folder where mlflow will log the experiments\n",
    "# autolog with mlflow\n",
    "mlflow_log_path = r'file:///' + mlflow_log_path  # note the tripple slashes\n",
    "print(mlflow_log_path)\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_log_path)  # tell mlflow where to log the experiments\n",
    "mlflow.pytorch.autolog()  # autolog with mlflow - log all the parameters, metrics, etc.\n",
    "\n",
    "exp_name = 'FMNIST_CNN'  # name of the experiment\n",
    "run_name = '1_layer'     # name of the run - multiple runs can be logged under the same experiment. Be sure to use different, clear, yet short names for different runs.\n",
    "\n",
    "\n",
    "# create model - 1 layer, cross-entropy loss, Adam optimizer\n",
    "n_hidden = []\n",
    "model = MyModel(n_input=784, n_hiddens=n_hidden, n_output=10)  # 784 input features for 28x28 images, 10 output classes\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "nested = True\n",
    "mlflow.set_experiment(exp_name)\n",
    "with mlflow.start_run(run_name=run_name, nested=nested):\n",
    "    mlflow.log_param('num_params', get_npars(model))  # you can log any parameters\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "        valid_loss, valid_acc = test(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Valid Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "        # Log metrics to MLflow at each iteration\n",
    "        mlflow.log_metric('train_loss', train_loss, step=epoch)\n",
    "        mlflow.log_metric('valid_loss', valid_loss, step=epoch)\n",
    "        mlflow.log_metric('train_acc', train_acc, step=epoch)\n",
    "        mlflow.log_metric('valid_acc', valid_acc, step=epoch)\n",
    "        \n",
    "    mlflow.pytorch.log_model(model, 'model_trained')  # you can save the model as a model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow server --host 127.0.0.1 --port 8080 --backend-store-uri file:///d:\\docs\\DSL\\Teaching\\CAS_AML\\CAS_AML_M3\\mlflow\n"
     ]
    }
   ],
   "source": [
    "# if run locally, use the following command from the terminal (copy & paste output to terminal)\n",
    "print(f'mlflow server --host 127.0.0.1 --port 8080 --backend-store-uri {mlflow_log_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On colab - copy the mlflow folder to local machine, and adjust the path to the mlflow folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run same with 2 layer NN (e.g. set n_hidden = [32] and a distinct run name)\n",
    "2. Copy the mlflow directory locally (or mount google drive)\n",
    "3. Then:\n",
    "    * explre the effect of n_hidden on the training time\n",
    "    * explore the effect of n_hidden on performance\n",
    "    * explore the effect of number of parameters vs performance\n",
    "    * ... explore :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_wrapper(num_epochs, run_name, n_hidden, lr, trial=None):\n",
    "    mlflow_log_path = root_path / 'mlflow'\n",
    "    mlflow_log_path.mkdir(exist_ok=True, parents=True)\n",
    "    mlflow_log_path =  str(mlflow_log_path)  # path to mlflow folder where mlflow will log the experiments\n",
    "    # autolog with mlflow\n",
    "    mlflow_log_path = r'file:///' + mlflow_log_path  # note the tripple slashes\n",
    "    print(mlflow_log_path)\n",
    "\n",
    "\n",
    "    mlflow.set_tracking_uri(mlflow_log_path)  # tell mlflow where to log the experiments\n",
    "    mlflow.pytorch.autolog()  # autolog with mlflow - log all the parameters, metrics, etc.\n",
    "\n",
    "    exp_name = 'FMNIST_CNN'  # name of the experiment\n",
    "    \n",
    "    # create model - 1 layer, cross-entropy loss, Adam optimizer\n",
    "    n_hidden = []\n",
    "    model = MyModel(n_input=784, n_hiddens=n_hidden, n_output=10)  # 784 input features for 28x28 images, 10 output classes\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    nested = True\n",
    "    mlflow.set_experiment(exp_name)\n",
    "    valid_acc_list = []\n",
    "    with mlflow.start_run(run_name=run_name, nested=nested):\n",
    "        mlflow.log_param('num_params', get_npars(model))  # you can log any parameters\n",
    "        mlflow.log_param('num_hidden', len(n_hidden))\n",
    "        mlflow.log_param('n_hidden', ','.join(n_hidden))\n",
    "\n",
    "        mlflow.log_param('lr', lr)\n",
    "        mlflow.log_param('num_epochs', num_epochs)\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "            valid_loss, valid_acc = test(model, valid_loader, criterion)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Valid Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "            # Log metrics to MLflow at each iteration\n",
    "            mlflow.log_metric('train_loss', train_loss, step=epoch)\n",
    "            mlflow.log_metric('valid_loss', valid_loss, step=epoch)\n",
    "            mlflow.log_metric('train_acc', train_acc, step=epoch)\n",
    "            mlflow.log_metric('valid_acc', valid_acc, step=epoch)\n",
    "\n",
    "            valid_acc_list.append(valid_acc)\n",
    "\n",
    "            # if trial is not None:\n",
    "            #     trial.report(valid_acc, epoch)\n",
    "\n",
    "            # # Handle pruning based on the intermediate value.\n",
    "            # if trial.should_prune():\n",
    "            #     raise optuna.TrialPruned()\n",
    "            \n",
    "        \n",
    "        mlflow.pytorch.log_model(model, 'model_trained')  # you can save the model as a model artifact\n",
    "    return np.max(valid_acc_list)  # best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use optuna to find the best parameters for the model - i.e. elements of the array n_hidden\n",
    "\n",
    "def objective(trial):\n",
    "    n_ep = 9\n",
    "    n_hidden_layers = trial.suggest_int(\"n_hidden\", 0, 3)\n",
    "    n_hidden = []\n",
    "    for i in range(n_hidden_layers):\n",
    "        # select between 16 and 2048 in log scale\n",
    "        n_neurons_log = trial.suggest_int(f\"n_hidden_{i}\", 4, 10)\n",
    "        n_neurons = 2 ** n_neurons_log\n",
    "        n_hidden.append(n_neurons)\n",
    "\n",
    "    #lr = trial.suggest_float(\"lr\", 3e-5, 3e-3, log=True)\n",
    "    lr = 1e-3\n",
    "\n",
    "    rname=f\"optuna2_{n_hidden_layers}_{n_hidden}_{lr}\"\n",
    "    acc = train_model_wrapper(num_epochs=n_ep, n_hidden=n_hidden, lr=lr, run_name=rname, trial=trial)\n",
    "\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicatedStudyError",
     "evalue": "Another study with name 'arch_opt2' already exists. Please specify a different name, or reuse the existing one by setting `load_if_exists` (for Python API) or `--skip-if-exists` flag (for CLI).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\default.py:924\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 924\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIntegrityError\u001b[0m: UNIQUE constraint failed: studies.study_name",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py:267\u001b[0m, in \u001b[0;36mRDBStorage.create_new_study\u001b[1;34m(self, directions, study_name)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _create_scoped_session(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoped_session) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m study_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py:76\u001b[0m, in \u001b[0;36m_create_scoped_session\u001b[1;34m(scoped_session, ignore_integrity_error)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m session\n\u001b[1;32m---> 76\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sqlalchemy_exc\u001b[38;5;241m.\u001b[39mIntegrityError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\session.py:2017\u001b[0m, in \u001b[0;36mSession.commit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2015\u001b[0m     trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autobegin_t()\n\u001b[1;32m-> 2017\u001b[0m \u001b[43mtrans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36mcommit\u001b[1;34m(self, _to_root)\u001b[0m\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\state_changes.py:139\u001b[0m, in \u001b[0;36m_StateChange.declare_states.<locals>._go\u001b[1;34m(fn, self, *arg, **kw)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     ret_value \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\session.py:1302\u001b[0m, in \u001b[0;36mSessionTransaction.commit\u001b[1;34m(self, _to_root)\u001b[0m\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expect_state(SessionTransactionState\u001b[38;5;241m.\u001b[39mPREPARED):\n\u001b[1;32m-> 1302\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnested:\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36m_prepare_impl\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\state_changes.py:139\u001b[0m, in \u001b[0;36m_StateChange.declare_states.<locals>._go\u001b[1;34m(fn, self, *arg, **kw)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     ret_value \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\session.py:1277\u001b[0m, in \u001b[0;36mSessionTransaction._prepare_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\session.py:4341\u001b[0m, in \u001b[0;36mSession.flush\u001b[1;34m(self, objects)\u001b[0m\n\u001b[0;32m   4340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flushing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 4341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4342\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\session.py:4476\u001b[0m, in \u001b[0;36mSession._flush\u001b[1;34m(self, objects)\u001b[0m\n\u001b[0;32m   4475\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m-> 4476\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m   4477\u001b[0m         transaction\u001b[38;5;241m.\u001b[39mrollback(_capture_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\session.py:4437\u001b[0m, in \u001b[0;36mSession._flush\u001b[1;34m(self, objects)\u001b[0m\n\u001b[0;32m   4436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4437\u001b[0m     \u001b[43mflush_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4438\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\unitofwork.py:466\u001b[0m, in \u001b[0;36mUOWTransaction.execute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m topological\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies, postsort_actions):\n\u001b[1;32m--> 466\u001b[0m     \u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\unitofwork.py:642\u001b[0m, in \u001b[0;36mSaveUpdateAll.execute\u001b[1;34m(self, uow)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;129m@util\u001b[39m\u001b[38;5;241m.\u001b[39mpreload_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlalchemy.orm.persistence\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, uow):\n\u001b[1;32m--> 642\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreloaded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morm_persistence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43muow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates_for_mapper_hierarchy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43muow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py:93\u001b[0m, in \u001b[0;36msave_obj\u001b[1;34m(base_mapper, states, uowtransaction, single)\u001b[0m\n\u001b[0;32m     85\u001b[0m     _emit_update_statements(\n\u001b[0;32m     86\u001b[0m         base_mapper,\n\u001b[0;32m     87\u001b[0m         uowtransaction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m         update,\n\u001b[0;32m     91\u001b[0m     )\n\u001b[1;32m---> 93\u001b[0m     \u001b[43m_emit_insert_statements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43muowtransaction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43minsert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m _finalize_insert_update_commands(\n\u001b[0;32m    102\u001b[0m     base_mapper,\n\u001b[0;32m    103\u001b[0m     uowtransaction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m     ),\n\u001b[0;32m    120\u001b[0m )\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py:1233\u001b[0m, in \u001b[0;36m_emit_insert_statements\u001b[1;34m(base_mapper, uowtransaction, mapper, table, insert, bookkeeping, use_orm_insert_stmt, execution_options)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1233\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m primary_key \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39minserted_primary_key\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1418\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\sql\\elements.py:515\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[1;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1640\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[1;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[0;32m   1632\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[0;32m   1633\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[0;32m   1634\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1638\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[0;32m   1639\u001b[0m )\n\u001b[1;32m-> 1640\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1846\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1986\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1986\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1987\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1988\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\base.py:2353\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2352\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\sqlalchemy\\engine\\default.py:924\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 924\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIntegrityError\u001b[0m: (sqlite3.IntegrityError) UNIQUE constraint failed: studies.study_name\n[SQL: INSERT INTO studies (study_name) VALUES (?)]\n[parameters: ('arch_opt2',)]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDuplicatedStudyError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m path_optuna_db \u001b[38;5;241m=\u001b[39m path_optuna_db_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb.sqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m path_optuna_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(path_optuna_db)\n\u001b[1;32m----> 6\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_optuna_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43march_opt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3600\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\optuna\\_convert_positional_args.py:83\u001b[0m, in \u001b[0;36mconvert_positional_args.<locals>.converter_decorator.<locals>.converter_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() got multiple values for arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicated_kwds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     81\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(inferred_kwargs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\optuna\\study\\study.py:1264\u001b[0m, in \u001b[0;36mcreate_study\u001b[1;34m(storage, sampler, pruner, study_name, direction, load_if_exists, directions)\u001b[0m\n\u001b[0;32m   1262\u001b[0m storage \u001b[38;5;241m=\u001b[39m storages\u001b[38;5;241m.\u001b[39mget_storage(storage)\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1264\u001b[0m     study_id \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_new_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirection_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDuplicatedStudyError:\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load_if_exists:\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\optuna\\storages\\_cached_storage.py:78\u001b[0m, in \u001b[0;36m_CachedStorage.create_new_study\u001b[1;34m(self, directions, study_name)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_new_study\u001b[39m(\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m, directions: Sequence[StudyDirection], study_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     77\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     study_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_new_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudy_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m     80\u001b[0m         study \u001b[38;5;241m=\u001b[39m _StudyInfo()\n",
      "File \u001b[1;32md:\\development\\Anaconda3\\envs\\py3.10_tf2.10\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py:279\u001b[0m, in \u001b[0;36mRDBStorage.create_new_study\u001b[1;34m(self, directions, study_name)\u001b[0m\n\u001b[0;32m    276\u001b[0m         session\u001b[38;5;241m.\u001b[39madd(models\u001b[38;5;241m.\u001b[39mStudyModel(study_name\u001b[38;5;241m=\u001b[39mstudy_name, directions\u001b[38;5;241m=\u001b[39mdirection_models))\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sqlalchemy_exc\u001b[38;5;241m.\u001b[39mIntegrityError:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m optuna\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mDuplicatedStudyError(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnother study with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify a different name, or reuse the existing one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby setting `load_if_exists` (for Python API) or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`--skip-if-exists` flag (for CLI).\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study_name)\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    286\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA new study created in RDB with name: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study_name))\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_study_id_from_name(study_name)\n",
      "\u001b[1;31mDuplicatedStudyError\u001b[0m: Another study with name 'arch_opt2' already exists. Please specify a different name, or reuse the existing one by setting `load_if_exists` (for Python API) or `--skip-if-exists` flag (for CLI)."
     ]
    }
   ],
   "source": [
    "path_optuna_db_dir = root_path / \"optuna_db\"\n",
    "path_optuna_db_dir.mkdir(exist_ok=True)\n",
    "path_optuna_db = path_optuna_db_dir / \"db.sqlite3\"\n",
    "path_optuna_db = \"sqlite:///\" + str(path_optuna_db)\n",
    "\n",
    "study = optuna.create_study(storage=path_optuna_db,\n",
    "                            direction=\"maximize\",\n",
    "                            study_name=\"architecture_opt\")\n",
    "\n",
    "study.optimize(objective, n_trials=120, timeout=3600*2, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optuna-dashboard sqlite:///d:\\docs\\DSL\\Teaching\\CAS_AML\\CAS_AML_M3\\optuna_db\\db.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# if run locally - paste the output of the print to the terminal:\n",
    "print(f'optuna-dashboard {path_optuna_db}')\n",
    "\n",
    "# otherwise copy file locally and adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
